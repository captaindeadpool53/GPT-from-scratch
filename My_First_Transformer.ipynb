{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNxogBFzWHEWPOSqDxAHWBt",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/captaindeadpool53/GPT-from-scratch/blob/main/My_First_Transformer.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4hH5Lequ5WKA",
        "outputId": "365bdf81-cbc3-499f-b7d6-51c688b4ad44"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-12 16:39:36--  https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1115394 (1.1M) [text/plain]\n",
            "Saving to: ‘input.txt.2’\n",
            "\n",
            "input.txt.2         100%[===================>]   1.06M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2025-05-12 16:39:37 (28.7 MB/s) - ‘input.txt.2’ saved [1115394/1115394]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "id": "OfM3wFis8xIx",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "f203e4ae-d0dc-467c-ff76-8d710926c479"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# read it in to inspect it\n",
        "with open('input.txt', 'r', encoding='utf-8') as f:\n",
        "    text = f.read()"
      ],
      "metadata": {
        "id": "TK2MTW2SzQc4"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jErGaPi_6G9m",
        "outputId": "62af7ed1-001e-404d-dbb1-81acaccbcf0b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1115394"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Sample text\n",
        "text[:1000]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 174
        },
        "id": "GtejyyaQ6cxR",
        "outputId": "9b6d6b55-2cba-47a2-defc-222d35dfbb14"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAll:\\nSpeak, speak.\\n\\nFirst Citizen:\\nYou are all resolved rather to die than to famish?\\n\\nAll:\\nResolved. resolved.\\n\\nFirst Citizen:\\nFirst, you know Caius Marcius is chief enemy to the people.\\n\\nAll:\\nWe know't, we know't.\\n\\nFirst Citizen:\\nLet us kill him, and we'll have corn at our own price.\\nIs't a verdict?\\n\\nAll:\\nNo more talking on't; let it be done: away, away!\\n\\nSecond Citizen:\\nOne word, good citizens.\\n\\nFirst Citizen:\\nWe are accounted poor citizens, the patricians good.\\nWhat authority surfeits on would relieve us: if they\\nwould yield us but the superfluity, while it were\\nwholesome, we might guess they relieved us humanely;\\nbut they think we are too dear: the leanness that\\nafflicts us, the object of our misery, is as an\\ninventory to particularise their abundance; our\\nsufferance is a gain to them Let us revenge this with\\nour pikes, ere we become rakes: for the gods know I\\nspeak this in hunger for bread, not in thirst for revenge.\\n\\n\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = sorted(set(text))\n",
        "print(''.join(vocab))\n",
        "print(len(vocab))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8T7gxtbL6gfp",
        "outputId": "5cfa2346-bdae-45fb-d3bb-4eac62b7ba6f"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n",
            "65\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenization\n",
        "stoi = { ch:i for i,ch in enumerate(vocab) }\n",
        "itos = { i:ch for i,ch in enumerate(vocab) }\n",
        "\n",
        "encode = lambda s: [stoi[c] for c in s] # encoder: take a string, output a list of integers\n",
        "decode = lambda l: \"\".join([itos[i] for i in l]) # decoder: take a list of integers, output a string\n"
      ],
      "metadata": {
        "id": "UlKc1A9x56GC"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(\"hi there SDFA\")));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o0jev8sq6yib",
        "outputId": "6454cabd-4dc4-430d-f048-d32f6bf65176"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hi there SDFA\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.manual_seed(1337)"
      ],
      "metadata": {
        "id": "ZNIBbdRX7HLY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c1771c2-97b3-4f48-d8a8-90906a19f03f"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7e382a97f530>"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Dataset generation\n",
        "completeData = torch.tensor(encode(text), dtype=torch.long)\n",
        "trainData = completeData[:int(0.9*len(completeData))]\n",
        "testData = completeData[int(0.9*len(completeData)):]\n",
        "\n",
        "def batchGenerator(is_training_data, BLOCK_SIZE, BATCH_SIZE):\n",
        "  dataset = trainData if is_training_data else testData\n",
        "  startingIndices = torch.randint(len(dataset) - BLOCK_SIZE, (BATCH_SIZE,))\n",
        "  xBatch = torch.stack([dataset[i:i+BLOCK_SIZE] for i in startingIndices], 0);\n",
        "  yBatch = torch.stack([dataset[i+1:i+1+BLOCK_SIZE] for i in startingIndices], 0);\n",
        "  xBatch, yBatch = xBatch.to(device), yBatch.to(device)\n",
        "  return xBatch, yBatch\n",
        "\n",
        "xBatch, yBatch = batchGenerator(True, 6, 4)"
      ],
      "metadata": {
        "id": "3UMh3d4O7DT_"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "xBatch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LBOKQtC8Fues",
        "outputId": "c52915bb-3483-4cdf-fbd6-cbda349da3fd"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[47, 58, 50, 43,  6,  1],\n",
              "        [21, 26, 15, 17,  1, 17],\n",
              "        [20, 27, 30, 31, 27, 26],\n",
              "        [58,  6,  1, 21,  1, 61]])"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yBatch"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E-GFFE--F8Z_",
        "outputId": "d0b6de33-abf0-4b23-9cc8-c124a6340f77"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[58, 50, 43,  6,  1, 61],\n",
              "        [26, 15, 17,  1, 17, 16],\n",
              "        [27, 30, 31, 27, 26, 10],\n",
              "        [ 6,  1, 21,  1, 61, 47]])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# The real stuff here\n",
        "\n",
        "class Transformer(nn.Module):\n",
        "  class AttentionHead(nn.Module):\n",
        "    def __init__(self, embedding_dims, head_dims):\n",
        "      super().__init__();\n",
        "      self.head_dims = head_dims\n",
        "\n",
        "      self.value_layer = nn.Linear(embedding_dims, head_dims)\n",
        "      self.query_layer = nn.Linear(embedding_dims, head_dims)\n",
        "      self.key_layer = nn.Linear(embedding_dims, head_dims)\n",
        "      self.register_buffer('tril', torch.tril(torch.ones(1000, 1000)))\n",
        "      self.dropout = nn.Dropout(DROPOUT)\n",
        "\n",
        "    def forward(self, input):\n",
        "      _, n_tokens, _ = input.shape # We only get n_tokens here so the model doesn't depend on n_tokens and it can be any number.\n",
        "\n",
        "      key = self.key_layer(input) #(B, T, C) input but layer applies to C dimension vector slices as a batch of (B*T).\n",
        "      query = self.query_layer(input)\n",
        "      value = self.value_layer(input)\n",
        "\n",
        "      weights = query@key.transpose(-2, -1) * (self.head_dims)**0.5                           # (B, T, hs) @ (B, hs, T) -> (B, T, T)\n",
        "      weights = weights.masked_fill(self.tril[:n_tokens, :n_tokens] == 0, float('-inf'))\n",
        "      weights = F.softmax(weights, -1)                                                        # Or affinities #Includes all columns like weights[i, j, :]\n",
        "\n",
        "      weights = self.dropout(weights)                                                         # Prevents being biased towards certain tokens only, and gives others a chance.\n",
        "      return weights@value # (B, T, hs)\n",
        "\n",
        "\n",
        "  class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, embedding_dims, n_heads):\n",
        "      super().__init__();\n",
        "\n",
        "      head_dims = embedding_dims//n_heads\n",
        "      self.heads = nn.ModuleList([Transformer.AttentionHead(embedding_dims, head_dims) for _ in range(n_heads)])\n",
        "      self.projection = nn.Linear(embedding_dims, embedding_dims) # Remixing of the head outputs\n",
        "      self.dropout = nn.Dropout(DROPOUT) # No activation due to residual connection\n",
        "\n",
        "    def forward(self, input):\n",
        "      output = torch.cat([head(input) for head in self.heads], -1) # Along channel dimension\n",
        "      output = self.projection(output)\n",
        "      output = self.dropout(output)\n",
        "\n",
        "      return output\n",
        "\n",
        "\n",
        "  class FeedForward(nn.Module):\n",
        "    def __init__(self, embedding_dims):\n",
        "      super().__init__();\n",
        "\n",
        "      self.network = nn.Sequential(\n",
        "          nn.Linear(embedding_dims, 4*embedding_dims),\n",
        "          nn.ReLU(),\n",
        "          nn.Linear(4*embedding_dims, embedding_dims),\n",
        "          nn.Dropout(DROPOUT) # No activation due to residual connection\n",
        "      )\n",
        "\n",
        "    def forward(self, input):\n",
        "      return self.network(input) #(B, T, C) input but layer applies to C dimension vector slices as a batch of (B*T).\n",
        "\n",
        "\n",
        "  class Block(nn.Module):\n",
        "    def __init__(self, embedding_dims, n_heads):\n",
        "      super().__init__();\n",
        "\n",
        "      self.layer_norm_sa = nn.LayerNorm(embedding_dims)\n",
        "      self.self_attention = Transformer.MultiHeadAttention(embedding_dims, n_heads)\n",
        "      self.layer_norm_ff = nn.LayerNorm(embedding_dims)\n",
        "      self.feed_forward = Transformer.FeedForward(embedding_dims)\n",
        "\n",
        "    def forward(self, input):\n",
        "      output = self.layer_norm_sa(input)\n",
        "      output = self.self_attention(output)\n",
        "      input = input + output\n",
        "      output = self.layer_norm_ff(input)\n",
        "      output = self.feed_forward(output)\n",
        "      input = input + output\n",
        "      return output\n",
        "\n",
        "\n",
        "  def __init__(self, vocab_size, embedding_dims, block_size, n_heads, n_blocks):\n",
        "    super().__init__();\n",
        "    self.block_size = block_size\n",
        "\n",
        "    self.token_embedding = nn.Embedding(vocab_size, embedding_dims);\n",
        "    self.positional_embedding = nn.Embedding(block_size, embedding_dims);\n",
        "    self.blocks = nn.Sequential(*[Transformer.Block(embedding_dims, n_heads) for _ in range(n_blocks)]);\n",
        "    self.layer_norm = nn.LayerNorm(embedding_dims)\n",
        "    self.linear = nn.Linear(embedding_dims, vocab_size);\n",
        "\n",
        "  def forward(self, input, expected_output = None):\n",
        "    _, T = input.shape\n",
        "\n",
        "    input_embedding = self.token_embedding(input)\n",
        "    input_positional_embedding = self.positional_embedding(torch.arange(T, device=device))\n",
        "\n",
        "    input_embedding = input_embedding + input_positional_embedding;\n",
        "    output = self.blocks(input_embedding)\n",
        "    output = self.layer_norm(output)\n",
        "    logits = self.linear(output) # Before softmax\n",
        "\n",
        "    if expected_output == None:\n",
        "      loss = None;\n",
        "    else:\n",
        "      B,T,C = logits.shape\n",
        "      logits = logits.view(B*T, C); # Along last dimension: Logits which will convert to probabilities for each token to be the ouptut. Index is the token value.\n",
        "      expected_output = expected_output.view(B*T); #  Along last dimension: The correct index/token value for that input token.\n",
        "      loss = F.cross_entropy(logits, expected_output) # Will calculate softmax of the input along channel dimension.\n",
        "    return logits, loss\n",
        "\n",
        "  def generate(self, input, tokens_to_generate):\n",
        "    for _ in range(tokens_to_generate):\n",
        "      input_in_context = input[ :, -self.block_size: ] # Crop last block_size tokens\n",
        "\n",
        "      logits, loss = self(input_in_context)\n",
        "      logits = logits[:,-1,:]\n",
        "      probabilities = F.softmax(logits, dim= -1)\n",
        "      predictions = torch.multinomial(probabilities, 1)\n",
        "      next_token = predictions\n",
        "\n",
        "      input = torch.cat([input, next_token], 1) # Add token to time sequence\n",
        "\n",
        "    return input"
      ],
      "metadata": {
        "id": "OLJwJ9HAN8-c"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Example of how multinomial samples from a distribution\n",
        "torch.multinomial(torch.tensor([[0.78, 0.88, 0.9],[0.2,0.2,0]], dtype=torch.float), 10, replacement=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mWtvrkeLXWN0",
        "outputId": "c71527b7-3dfa-470f-d4d1-83d22a1e2fb3"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[1, 2, 1, 2, 1, 0, 2, 0, 0, 0],\n",
              "        [0, 0, 1, 0, 0, 1, 0, 1, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Constants and hyperparameters\n",
        "\n",
        "DROPOUT = 0.03 #Fixed\n",
        "\n",
        "BLOCK_SIZE = 400 # Number of tokens passed / Context length\n",
        "BATCH_SIZE = 32\n",
        "VOCAB_SIZE = len(vocab)\n",
        "EMBEDDING_DIMS = 162\n",
        "NUMBER_OF_BLOCKS = 6\n",
        "NUMBER_OF_HEADS = 6\n",
        "EPOCHS = 300\n",
        "LEARNING_RATE = 3e-4\n",
        "\n",
        "TRAINING_DATA = True\n",
        "TEST_DATA = False"
      ],
      "metadata": {
        "id": "elTbc19p9KVk"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialise model\n",
        "model = Transformer(VOCAB_SIZE, EMBEDDING_DIMS, BLOCK_SIZE, NUMBER_OF_HEADS, NUMBER_OF_BLOCKS);\n",
        "\n",
        "model = model.to(device)\n",
        "\n",
        "# print the number of parameters in the model\n",
        "print(sum(p.numel() for p in model.parameters()), 'parameters')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9GIM2MTM_-lj",
        "outputId": "4f1b6170-0efe-4222-d397-88338e481060"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1988453 parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = torch.optim.AdamW(model.parameters(), lr=LEARNING_RATE)"
      ],
      "metadata": {
        "id": "vSut5fg2DDMS"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the checkpoint\n",
        "checkpoint = torch.load('checkpoint.pth', weights_only=False)\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "last_epoch = checkpoint.get('epoch', 0)\n",
        "last_loss = checkpoint.get('loss', None)"
      ],
      "metadata": {
        "id": "bkyZk6mWQDqg"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def calculate_test_loss(model):\n",
        "  model.eval()\n",
        "\n",
        "  total_loss = 0;\n",
        "\n",
        "  for _ in range(20): # Calculate loss for a few batches\n",
        "    x, y = batchGenerator(TEST_DATA, BLOCK_SIZE, BATCH_SIZE)\n",
        "    logits, loss = model(x, y)\n",
        "    total_loss += loss\n",
        "\n",
        "  model.train()\n",
        "  return total_loss/20\n"
      ],
      "metadata": {
        "id": "yPMe3s4KASfR"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training loop\n",
        "\n",
        "for i in range(EPOCHS):\n",
        "  xBatch, yBatch = batchGenerator(TRAINING_DATA, BLOCK_SIZE, BATCH_SIZE)\n",
        "  logits, loss = model(xBatch, yBatch)\n",
        "  optimizer.zero_grad(set_to_none=True)\n",
        "  loss.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if i%10==0:\n",
        "    validation_loss = calculate_test_loss(model)\n",
        "    print(f\"train loss:  {loss} , val loss: {validation_loss}, iteration: {i}\")"
      ],
      "metadata": {
        "id": "PhHiOuqhGD-B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "32bc202d-b77f-4260-934a-ff6b3c20cfb8"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train loss:  4.440652847290039 , val loss: 3.8740177154541016, iteration: 0\n",
            "train loss:  3.3981051445007324 , val loss: 3.3871586322784424, iteration: 10\n",
            "train loss:  3.3797905445098877 , val loss: 3.3400306701660156, iteration: 20\n",
            "train loss:  3.3238539695739746 , val loss: 3.317779541015625, iteration: 30\n",
            "train loss:  3.3094658851623535 , val loss: 3.318784713745117, iteration: 40\n",
            "train loss:  3.354759454727173 , val loss: 3.3144233226776123, iteration: 50\n",
            "train loss:  3.313220262527466 , val loss: 3.311138153076172, iteration: 60\n",
            "train loss:  3.3437087535858154 , val loss: 3.313133955001831, iteration: 70\n",
            "train loss:  3.2668542861938477 , val loss: 3.3119988441467285, iteration: 80\n",
            "train loss:  3.313037157058716 , val loss: 3.2729663848876953, iteration: 90\n",
            "train loss:  3.209226608276367 , val loss: 3.21709942817688, iteration: 100\n",
            "train loss:  3.1587743759155273 , val loss: 3.1571452617645264, iteration: 110\n",
            "train loss:  3.0935981273651123 , val loss: 3.064011335372925, iteration: 120\n",
            "train loss:  3.017376184463501 , val loss: 2.9896399974823, iteration: 130\n",
            "train loss:  2.9731030464172363 , val loss: 2.9118876457214355, iteration: 140\n",
            "train loss:  2.824476718902588 , val loss: 2.8411900997161865, iteration: 150\n",
            "train loss:  2.7923662662506104 , val loss: 2.762359857559204, iteration: 160\n",
            "train loss:  2.752213716506958 , val loss: 2.7160258293151855, iteration: 170\n",
            "train loss:  2.71714448928833 , val loss: 2.6872589588165283, iteration: 180\n",
            "train loss:  2.652501106262207 , val loss: 2.6597907543182373, iteration: 190\n",
            "train loss:  2.6585745811462402 , val loss: 2.643291711807251, iteration: 200\n",
            "train loss:  2.6435110569000244 , val loss: 2.622697353363037, iteration: 210\n",
            "train loss:  2.6364972591400146 , val loss: 2.6102449893951416, iteration: 220\n",
            "train loss:  2.6080689430236816 , val loss: 2.598231077194214, iteration: 230\n",
            "train loss:  2.6057541370391846 , val loss: 2.576795816421509, iteration: 240\n",
            "train loss:  2.5608837604522705 , val loss: 2.5643537044525146, iteration: 250\n",
            "train loss:  2.591778039932251 , val loss: 2.559598922729492, iteration: 260\n",
            "train loss:  2.5476818084716797 , val loss: 2.5451319217681885, iteration: 270\n",
            "train loss:  2.5417189598083496 , val loss: 2.540024518966675, iteration: 280\n",
            "train loss:  2.5518076419830322 , val loss: 2.5422873497009277, iteration: 290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing the learning rate\n",
        "\n",
        "for param_group in optimizer.param_groups:\n",
        "    param_group['lr'] = 0.00001"
      ],
      "metadata": {
        "id": "hK08BKziQEwz"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving Checkpoint\n",
        "torch.save({\n",
        "    'model_state_dict': model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'epoch': 600,\n",
        "    'loss': 2.511981248855591\n",
        "}, 'checkpoint.pth')"
      ],
      "metadata": {
        "id": "rx7KbR40QAgx"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Clearing memory if needed\n",
        "\n",
        "import gc\n",
        "\n",
        "del model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hLWrQQttUL-H",
        "outputId": "67ea2bf7-cf3e-4b9d-8527-b10547ba1e35"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "5742"
            ]
          },
          "metadata": {},
          "execution_count": 57
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate from the dummy model\n",
        "TOKENS_TO_GENERATE = 500\n",
        "\n",
        "dummy_model = Transformer(VOCAB_SIZE, EMBEDDING_DIMS, BLOCK_SIZE, NUMBER_OF_HEADS, NUMBER_OF_BLOCKS);\n",
        "\n",
        "prompt = encode(\"First Citizen:\\nBefore we proceed any further\")\n",
        "prompt = torch.unsqueeze(torch.tensor(prompt, dtype= torch.long, device= device), 0) # Convert to tensor and add a batch dimension\n",
        "output = dummy_model.generate(prompt, TOKENS_TO_GENERATE)[0].tolist() # Take first batch\n",
        "\n",
        "print(\"Untrained: \");\n",
        "print(decode(output));"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Cm3G1uFl3Sk6",
        "outputId": "5ab42bdf-21b1-49fb-9b36-6bff24776904"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Untrained: \n",
            "First Citizen:\n",
            "Before we proceed any furtherjQHK-YDtTOZTYvTCk!lZVt-;:H!wFnwKgv?g.NgVqGmE? O\n",
            "dB&Me\n",
            "TLyHK.\n",
            "kIYbUcnhBUklz: :n;!zfH!W?lwgfA!UCWW FchWufmKhL,p!?-o.zTwQz3PLf&V;BzGNMeh!kq:-YclH Yn!pUI'gd?LgKdWv;IPKqW,gvT\n",
            "FJiKSW!s;,K;PYHYpWR.CT.V J&TMUTz;'bTPw3'KKWgS;,ZlQtd&kgm\n",
            ".lA?OYWvmKznFGrY'vO. .n&PEJOyJ r''oy.f'UL3Oc,C,WYhK;JTq.jPBAZLWdrEYWNdczPzrm?KvEY;K WfmnlYcfbugzkDnFOE TtTGKcXmPfHagssh\n",
            "\n",
            "!GN nZFKAAth nazk3l?zjW c.ztI!g'wLw mTzDkwGTKzLN.VhXhFn O;.z3bPJwmWxg!amnEYthQN!hlETUN!JUW zv:!PZOwb'NYNwdwYYpWPemJ'GWfswzcjMYLW!\n",
            "Yzl\n",
            "k-OH  n&Wm;a$ pbwW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate from the model\n",
        "TOKENS_TO_GENERATE = 500\n",
        "\n",
        "prompt = encode(\"First Citizen:\\nBefore we proceed any further\")\n",
        "prompt = torch.unsqueeze(torch.tensor(prompt, dtype= torch.long, device= device), 0) # Convert to tensor and add a batch dimension\n",
        "output = model.generate(prompt, TOKENS_TO_GENERATE)[0].tolist() # Take first batch\n",
        "\n",
        "print(\"After training: \");\n",
        "print(decode(output)); #"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A2SYM-rFLCw9",
        "outputId": "31a299dc-5eec-460f-df02-0cff203df28f"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "After training: \n",
            "First Citizen:\n",
            "Before we proceed any further s arsN: fsh\n",
            "LSdpourithimend yoliy, mpyooceatery mau y apitheris tomy\n",
            "Y toulis I'lllla goun ares kw- is va.\n",
            "Hore, wht byitoncou or h t uine didse, o is g\n",
            "\n",
            "HBllticenan?\n",
            "CI that d l\n",
            "vyet t hrd med o bent, tho outor esowes\n",
            "Ifor, hst boucethyimesonool E, le fky ien iman u heth Foyor-ors we tangoftould:\n",
            "RGetr s y aalvaLAn t oritherare Wo fit ather pt h bys u puy ertkugha d, histhe towe my yot,\n",
            "Irou\n",
            "Sue ce llis ncury cnes\n",
            "MNS! s ghy c won,\n",
            "TIul, wirthers hir, ma hin to ie sn.\n",
            "ig C:\n",
            "\n",
            "h wid lMo ar bfino\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "### Not that coherent, but more language-like. This is because of the trade-off between vocabulary size and the actual meaningful context that can be passed to the model. Although I did stop the training early and it can be optimised more even with this tokenization."
      ],
      "metadata": {
        "id": "gMcDIM1Wt4KN"
      }
    }
  ]
}
